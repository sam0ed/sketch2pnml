{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8492ae",
   "metadata": {},
   "source": [
    "### commons.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "9c2d01ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def is_number(n):\n",
    "    is_number = True\n",
    "    try:\n",
    "        num = float(n)\n",
    "        # check for \"nan\" floats\n",
    "        is_number = num == num   # or use `math.isnan(num)`\n",
    "    except ValueError:\n",
    "        is_number = False\n",
    "    return is_number\n",
    "\n",
    "def minmaxToContours(xyxy):\n",
    "    x1, y1, x2, y2 = xyxy\n",
    "    return np.array([[x1, y1], [x2, y1], [x2, y2], [x1, y2]], dtype=np.int32)  # Convert to contour format\n",
    "\n",
    "def fill_contours(preprocessed_img, contours):\n",
    "    \"\"\"Fills the contours in the preprocessed image.\"\"\"\n",
    "    img_filled = preprocessed_img.copy()\n",
    "    \n",
    "    cv2.drawContours(img_filled, contours, -1, (255), thickness=cv2.FILLED)\n",
    "    return img_filled   \n",
    "\n",
    "def remove_contours(preprocessed_img, contours):\n",
    "    \"\"\"Removes the detected node shapes (circles and rectangles) from the image.\"\"\"\n",
    "\n",
    "    contours_mask = np.zeros_like(preprocessed_img)\n",
    "    contours_mask = fill_contours(contours_mask, contours)\n",
    "    contours_mask = cv2.bitwise_not(contours_mask)\n",
    "\n",
    "    img_no_contours = cv2.bitwise_and(preprocessed_img, contours_mask)\n",
    "    return img_no_contours\n",
    "\n",
    "def filter_enclosed_contours(\n",
    "    contours_to_filter: list[np.ndarray],\n",
    "    enclosing_contours_input: list[np.ndarray],\n",
    "    include_border: bool = True\n",
    ") -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Filters contours from contours_to_filter that are entirely enclosed by ANY contour\n",
    "    in enclosing_contours_input.\n",
    "\n",
    "    Args:\n",
    "        contours_to_filter: List of contours (np.ndarray) to be filtered.\n",
    "                            Each contour is typically an array of shape (N, 1, 2).\n",
    "        enclosing_contours_input: List of contours (np.ndarray) that act as enclosing shapes.\n",
    "                                  Each contour is typically an array of shape (M, 1, 2).\n",
    "        include_border: Whether points on the border of an enclosing_contour\n",
    "                        count as inside.\n",
    "\n",
    "    Returns:\n",
    "        A list of contours from contours_to_filter that are fully enclosed.\n",
    "        Each contour in the returned list is a reference to an object in \n",
    "        the input contours_to_filter list.\n",
    "    \"\"\"\n",
    "    if not contours_to_filter or not enclosing_contours_input:\n",
    "        return []\n",
    "\n",
    "    # Filter out empty enclosing contours and precompute their bounding rects\n",
    "    enclosing_contours = []\n",
    "    brects_enclosing = []\n",
    "    for c_enclosing in enclosing_contours_input:\n",
    "        # Ensure contour has points (shape[0] is the number of points)\n",
    "        if c_enclosing.shape[0] > 0:\n",
    "            enclosing_contours.append(c_enclosing)\n",
    "            brects_enclosing.append(cv2.boundingRect(c_enclosing))\n",
    "\n",
    "    if not enclosing_contours: # No valid enclosing contours\n",
    "        return []\n",
    "\n",
    "    result_contours = []\n",
    "    # Keep track of indices of contours from contours_to_filter that have been added\n",
    "    added_contour_indices = set()\n",
    "\n",
    "    test_threshold = 0 if include_border else 1\n",
    "\n",
    "    # Precompute bounding rects for contours_to_filter\n",
    "    # (x, y, w, h)\n",
    "    brects_to_filter = []\n",
    "    for c in contours_to_filter:\n",
    "        if c.shape[0] > 0:\n",
    "            brects_to_filter.append(cv2.boundingRect(c))\n",
    "        else:\n",
    "            # Placeholder for empty contours, they will be skipped later\n",
    "            brects_to_filter.append((0,0,0,0))\n",
    "\n",
    "\n",
    "    for idx1, contour1 in enumerate(contours_to_filter):\n",
    "        if idx1 in added_contour_indices:\n",
    "            continue\n",
    "\n",
    "        # An empty contour (no points) cannot be considered enclosed\n",
    "        if contour1.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        x1, y1, w1, h1 = brects_to_filter[idx1]\n",
    "        is_contour1_enclosed_by_any = False\n",
    "\n",
    "        for idx2, contour2 in enumerate(enclosing_contours):\n",
    "            x2, y2, w2, h2 = brects_enclosing[idx2]\n",
    "\n",
    "            # AABB Pruning: For contour1 to be enclosed by contour2,\n",
    "            # contour1's bounding box must be within contour2's bounding box.\n",
    "            if not (x1 >= x2 and \\\n",
    "                    y1 >= y2 and \\\n",
    "                    (x1 + w1) <= (x2 + w2) and \\\n",
    "                    (y1 + h1) <= (y2 + h2)):\n",
    "                continue # Bounding box of contour1 is not contained in bounding box of contour2\n",
    "\n",
    "            # Precise point-in-polygon test:\n",
    "            # All points of contour1 must be inside (or on border of) contour2\n",
    "            all_points_inside = True\n",
    "            # Reshape contour1 from (N,1,2) to (N,2) for easier iteration\n",
    "            points_contour1 = contour1.reshape(-1, 2)\n",
    "\n",
    "            for pt_x, pt_y in points_contour1:\n",
    "                # cv2.pointPolygonTest expects point as (float, float)\n",
    "                dist = cv2.pointPolygonTest(contour2, (float(pt_x), float(pt_y)), False)\n",
    "                if dist < test_threshold:\n",
    "                    all_points_inside = False\n",
    "                    break # This point of contour1 is outside contour2\n",
    "\n",
    "            if all_points_inside:\n",
    "                is_contour1_enclosed_by_any = True\n",
    "                break # contour1 is enclosed by contour2; no need to check other enclosing_contours\n",
    "\n",
    "        if is_contour1_enclosed_by_any:\n",
    "            result_contours.append(contour1)\n",
    "            added_contour_indices.add(idx1)\n",
    "\n",
    "    return result_contours\n",
    "\n",
    "def dilate_contour(contour, image_shape, config):\n",
    "    if contour is None or len(contour) == 0:\n",
    "        raise ValueError(\"Invalid contour provided for dilation.\")\n",
    "    \n",
    "    dilation_kernel_size = config.get('shape_detection', {}).get('remove_nodes_dilation_kernel_size', [3, 3])\n",
    "    dilation_iterations = config.get('shape_detection', {}).get('remove_nodes_dilation_iterations', 3)\n",
    "\n",
    "\n",
    "    height, width = image_shape\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    cv2.drawContours(mask, [contour], -1, (255), thickness=cv2.FILLED)\n",
    "\n",
    "    kernel = np.ones((dilation_kernel_size[0], dilation_kernel_size[1]), np.uint8)\n",
    "    dilated_mask = cv2.dilate(mask, kernel, iterations=dilation_iterations)\n",
    "    dilated_contours, _ = cv2.findContours(dilated_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if dilated_contours:\n",
    "        largest_dilated_contour = max(dilated_contours, key=cv2.contourArea)\n",
    "        return largest_dilated_contour\n",
    "    else:\n",
    "        raise ValueError(\"No contours found after dilation.\")\n",
    "    \n",
    "def find_closest_distance_to_contour(point_obj: Point, contour_np: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the shortest distance from a Point object to a contour.\n",
    "    The contour is a numpy array of points.\n",
    "    - If contour has 1 point, it's point-to-point distance.\n",
    "    - If contour has 2 points, it's distance to the line segment defined by these points.\n",
    "    - If contour has >2 points, it's distance to the boundary of the polygon defined by these points.\n",
    "\n",
    "    Args:\n",
    "        point_obj: The Point object from which to measure the distance.\n",
    "        contour_np: A NumPy array representing the contour, shape (N, 1, 2) or (N, 2).\n",
    "                    Coordinates are typically integers.\n",
    "\n",
    "    Returns:\n",
    "        The shortest distance as a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate and standardize contour_np shape for point extraction\n",
    "    if contour_np.ndim == 3 and contour_np.shape[1] == 1 and contour_np.shape[2] == 2:\n",
    "        # Shape (N, 1, 2), reshape to (N, 2) for easier iteration\n",
    "        processed_contour_points = contour_np.reshape(-1, 2)\n",
    "    elif contour_np.ndim == 2 and contour_np.shape[1] == 2:\n",
    "        # Shape (N, 2), use as is\n",
    "        processed_contour_points = contour_np\n",
    "    else:\n",
    "        raise ValueError(f\"Contour numpy array has an unsupported shape: {contour_np.shape}. \"\n",
    "                         \"Expected (N, 1, 2) or (N, 2).\")\n",
    "\n",
    "    num_contour_points = processed_contour_points.shape[0]\n",
    "\n",
    "    if num_contour_points == 0:\n",
    "        return float('inf') # No points in contour, distance is infinite\n",
    "\n",
    "    if num_contour_points == 1:\n",
    "        contour_pt_coords = processed_contour_points[0]\n",
    "        contour_pt_obj = Point(contour_pt_coords[0], contour_pt_coords[1])\n",
    "        return point_obj.get_distance_between_points(contour_pt_obj)\n",
    "\n",
    "    if num_contour_points == 2:\n",
    "        pt_a_coords = processed_contour_points[0]\n",
    "        pt_b_coords = processed_contour_points[1]\n",
    "        \n",
    "        # Create Point objects for the segment endpoints\n",
    "        segment_pt_a = Point(pt_a_coords[0], pt_a_coords[1])\n",
    "        segment_pt_b = Point(pt_b_coords[0], pt_b_coords[1])\n",
    "        \n",
    "        line_segment = Line(segment_pt_a, segment_pt_b)\n",
    "        return line_segment.distance_point_to_segment(point_obj)\n",
    "    \n",
    "    # num_contour_points > 2 (Polygon case)\n",
    "    else:\n",
    "        # cv2.pointPolygonTest requires contour in (N, 1, 2) format and float32 type.\n",
    "        # We use the original contour_np for this, as it might already be (N,1,2).\n",
    "        if contour_np.ndim == 2: # Original was (N,2)\n",
    "            contour_for_cv2 = contour_np.reshape((-1, 1, 2)).astype(np.float32)\n",
    "        else: # Original was (N,1,2)\n",
    "            contour_for_cv2 = contour_np.astype(np.float32)\n",
    "            \n",
    "        # The query point for pointPolygonTest needs to be a float tuple\n",
    "        query_point_tuple = (float(point_obj.x), float(point_obj.y))\n",
    "        \n",
    "        # measureDist=True returns signed distance:\n",
    "        # The absolute value is the shortest distance to any edge of the contour.\n",
    "        distance = cv2.pointPolygonTest(contour_for_cv2, query_point_tuple, True)\n",
    "        return abs(distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde9e917",
   "metadata": {},
   "source": [
    "### renderer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbcb0a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "724943e9",
   "metadata": {},
   "source": [
    "### recognize_text.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "fad86cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from doctr.models import ocr_predictor\n",
    "from models import Text # Import the Text class\n",
    "\n",
    "def _geometry_to_absolute_coords(relative_geom: tuple[tuple[float, float], tuple[float, float]], \n",
    "                                 img_width: int, img_height: int) -> tuple[tuple[int, int], tuple[int, int]]:\n",
    "    \"\"\"Converts doctr's relative coordinates to absolute integer coordinates.\"\"\"\n",
    "    (xmin_rel, ymin_rel), (xmax_rel, ymax_rel) = relative_geom\n",
    "    \n",
    "    xmin_abs = int(xmin_rel * img_width)\n",
    "    ymin_abs = int(ymin_rel * img_height)\n",
    "    xmax_abs = int(xmax_rel * img_width)\n",
    "    ymax_abs = int(ymax_rel * img_height)\n",
    "    \n",
    "    return ((xmin_abs, ymin_abs), (xmax_abs, ymax_abs))\n",
    "\n",
    "def detect_text(img_color_resized: np.ndarray, config: dict) -> list[Text]:\n",
    "    \"\"\"\n",
    "    Detects text using doctr and returns a list of Text objects with absolute coordinates.\n",
    "    (Implementation remains the same)\n",
    "    \"\"\"\n",
    "    predictor_params = config.get('text_detection', {})\n",
    "    predictor = ocr_predictor(\n",
    "        det_arch='db_resnet50', \n",
    "        reco_arch='crnn_vgg16_bn', \n",
    "        pretrained=True,\n",
    "    )\n",
    "    predictor.det_predictor.model.postprocessor.bin_thresh = predictor_params.get('bin_thresh', 0.3)\n",
    "    predictor.det_predictor.model.postprocessor.box_thresh = predictor_params.get('box_thresh', 0.1)\n",
    "\n",
    "    out = predictor([img_color_resized])\n",
    "\n",
    "    detected_texts: list[Text] = []\n",
    "    img_height, img_width = img_color_resized.shape[:2]\n",
    "\n",
    "    if out.pages:\n",
    "        for block in out.pages[0].blocks:\n",
    "            for line in block.lines:\n",
    "                for word in line.words:\n",
    "                    abs_geom = _geometry_to_absolute_coords(word.geometry, img_width, img_height)\n",
    "                    text_obj = Text(value=word.value, \n",
    "                                      geometry_abs=abs_geom, \n",
    "                                      confidence=word.confidence)\n",
    "                    detected_texts.append(text_obj)\n",
    "    \n",
    "    return detected_texts\n",
    "\n",
    "def get_img_no_text(preprocessed_img: np.ndarray, detected_texts: list[Text]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Removes text from the preprocessed image by finding contours within text bounding boxes\n",
    "    and applying a mask using bitwise_and (similar to original notebook).\n",
    "\n",
    "    Args:\n",
    "        preprocessed_img: The thresholded image (e.g., from Otsu).\n",
    "        detected_texts: List of Text objects with absolute coordinates.\n",
    "\n",
    "    Returns:\n",
    "        The image with text contours removed (blacked out).\n",
    "    \"\"\"\n",
    "    if not detected_texts:\n",
    "        ### throw an error\n",
    "        raise ValueError(\"No detected texts to process.\")\n",
    "\n",
    "    img_contours_list, _ = cv2.findContours(preprocessed_img, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    bbox_to_contours = [minmaxToContours([text.pt1.x, text.pt1.y, text.pt2.x, text.pt2.y])  for text in detected_texts]\n",
    "\n",
    "    text_contours = filter_enclosed_contours(img_contours_list, bbox_to_contours, include_border=True)\n",
    "\n",
    "    img_no_text = remove_contours(preprocessed_img, text_contours)\n",
    "\n",
    "    return img_no_text \n",
    "\n",
    "# Helper function to get the center of a Text object's bounding box\n",
    "def get_text_center(text_obj: Text) -> Point:\n",
    "    \"\"\"Calculates the center point of a Text object's bounding box.\"\"\"\n",
    "    center_x = (text_obj.pt1.x + text_obj.pt2.x) / 2.0\n",
    "    center_y = (text_obj.pt1.y + text_obj.pt2.y) / 2.0\n",
    "    return Point(center_x, center_y)\n",
    "\n",
    "\n",
    "def link_text_to_elements(\n",
    "    detected_text_list: list[Text],\n",
    "    places_list: list[Place],\n",
    "    transitions_list: list[Transition],\n",
    "    arcs_list: list[Arc],\n",
    "    distance_threshold: float\n",
    "):\n",
    "    \"\"\"\n",
    "    Associates Text objects with the closest Place, Transition, or Arc\n",
    "    if the distance from the text's center to the element is within a given threshold.\n",
    "    The association is done by appending the Text object to the `text` list\n",
    "    of the corresponding Place, Transition, or Arc. \n",
    "    \"\"\"\n",
    "\n",
    "    # Clear any previous text associations from elements\n",
    "    for element_list in [places_list, transitions_list, arcs_list]:\n",
    "        for element in element_list:\n",
    "            element.text = [] \n",
    "\n",
    "    for text_obj in detected_text_list:\n",
    "        text_center = get_text_center(text_obj)\n",
    "        \n",
    "        min_overall_distance = float('inf')\n",
    "        closest_element_overall = None\n",
    "\n",
    "        # 1. Check Places\n",
    "        for place in places_list:\n",
    "            dist_to_place_center = text_center.get_distance_between_points(place.center)\n",
    "            distance = max(0, dist_to_place_center - place.radius) # Distance to circumference\n",
    "            \n",
    "            if distance < min_overall_distance:\n",
    "                min_overall_distance = distance\n",
    "                closest_element_overall = place\n",
    "\n",
    "        # 2. Check Transitions\n",
    "        for transition in transitions_list:\n",
    "            contour_to_use = None\n",
    "            if transition.original_detection_data is not None and \\\n",
    "               isinstance(transition.original_detection_data, np.ndarray) and \\\n",
    "               transition.original_detection_data.shape[0] > 0:\n",
    "                contour_to_use = transition.original_detection_data\n",
    "            elif transition.points and len(transition.points) > 0: # Fallback to box_points\n",
    "                contour_to_use = np.array([p.get_numpy_array() for p in transition.points], dtype=np.int32).reshape((-1, 1, 2))\n",
    "\n",
    "            if contour_to_use is None or contour_to_use.shape[0] == 0:\n",
    "                continue \n",
    "                \n",
    "            distance = find_closest_distance_to_contour(text_center, contour_to_use)\n",
    "            if distance < min_overall_distance:\n",
    "                min_overall_distance = distance\n",
    "                closest_element_overall = transition\n",
    "        \n",
    "        # 3. Check Arcs\n",
    "        for arc in arcs_list:\n",
    "            arc_contour_for_dist_calc = None\n",
    "            # Prioritize arc.points if available, as it represents the path\n",
    "            if arc.points and len(arc.points) >= 1:\n",
    "                arc_contour_for_dist_calc = np.array([p.get_numpy_array() for p in arc.points], dtype=np.int32).reshape((-1, 1, 2))\n",
    "            # Fallback if arc.points is empty but start/end points are defined (simple line arc)\n",
    "            elif arc.start_point and arc.end_point:\n",
    "                arc_contour_for_dist_calc = np.array([\n",
    "                    arc.start_point.get_numpy_array(), \n",
    "                    arc.end_point.get_numpy_array()\n",
    "                ], dtype=np.int32).reshape((-1, 1, 2))\n",
    "            # If arc is defined by arc.lines (more complex, potentially disjoint segments)\n",
    "            # This path is less common if arc.points is expected to be canonical.\n",
    "            elif arc.lines:\n",
    "                current_arc_min_dist_lines = float('inf')\n",
    "                for line_segment in arc.lines:\n",
    "                    dist_to_segment = line_segment.distance_point_to_segment(text_center)\n",
    "                    current_arc_min_dist_lines = min(current_arc_min_dist_lines, dist_to_segment)\n",
    "                \n",
    "                if current_arc_min_dist_lines < min_overall_distance:\n",
    "                    min_overall_distance = current_arc_min_dist_lines\n",
    "                    closest_element_overall = arc\n",
    "                continue # Skip contour-based distance if lines were processed\n",
    "\n",
    "            if arc_contour_for_dist_calc is not None and arc_contour_for_dist_calc.shape[0] > 0:\n",
    "                distance = find_closest_distance_to_contour(text_center, arc_contour_for_dist_calc)\n",
    "                if distance < min_overall_distance:\n",
    "                    min_overall_distance = distance\n",
    "                    closest_element_overall = arc\n",
    "        \n",
    "        # Associate text with the overall closest element if within threshold\n",
    "        if closest_element_overall is not None and min_overall_distance <= distance_threshold:\n",
    "            closest_element_overall.text.append(text_obj)\n",
    "            # print(f\"Associated '{text_obj.value}' (center: {text_center}) with {closest_element_overall.__class__.__name__} id={id(closest_element_overall)} (dist: {min_overall_distance:.2f})\")\n",
    "        # else:\n",
    "            # print(f\"Text '{text_obj.value}' (center: {text_center}) not associated, min_dist {min_overall_distance:.2f} > threshold {distance_threshold}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1204c6c4",
   "metadata": {},
   "source": [
    "### recognize_node.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "5e29b036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import largestinteriorrectangle as lir\n",
    "from constants import EPS\n",
    "\n",
    "def get_circle_overlap(contour):\n",
    "    \"\"\"Checks if a contour is roughly circular based on the ratio of its area to its minimum enclosing circle.\"\"\"\n",
    "    (x, y), radius = cv2.minEnclosingCircle(contour)\n",
    "    enclosing_area = np.pi * (radius ** 2) + EPS\n",
    "    contour_area = cv2.contourArea(contour)\n",
    "    \n",
    "    return contour_area / enclosing_area \n",
    "\n",
    "def get_rectangle_overlap(contour):\n",
    "    \"\"\"Checks if a contour is roughly rectangular based on the ratio of its area to its minimum area bounding box.\"\"\"\n",
    "    rect = cv2.minAreaRect(contour)\n",
    "    box_area = rect[1][0] * rect[1][1] + EPS\n",
    "    contour_area = cv2.contourArea(contour)\n",
    "        \n",
    "    return contour_area / box_area \n",
    "\n",
    "def detect_shapes(preprocessed_img, config):\n",
    "\n",
    "    circle_threshold = config.get('shape_detection', {}).get('fill_circle_enclosing_threshold', 0.8)\n",
    "    rect_threshold = config.get('shape_detection', {}).get('fill_rect_enclosing_threshold', 0.95)\n",
    "    \n",
    "    contours_list, hierarchy = cv2.findContours(preprocessed_img, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE) \n",
    "\n",
    "    circles = []\n",
    "    rectangles = []\n",
    "    for contour in contours_list:\n",
    "        circle_overlap_percentage = get_circle_overlap(contour)\n",
    "        rectangle_overlap_percentage = get_rectangle_overlap(contour)\n",
    "\n",
    "        if circle_overlap_percentage > rectangle_overlap_percentage and circle_overlap_percentage > circle_threshold:\n",
    "            circles.append(contour)\n",
    "            \n",
    "        elif rectangle_overlap_percentage > circle_overlap_percentage and rectangle_overlap_percentage > rect_threshold:\n",
    "            # print(f\"Rectangle detected: rectangle_overlap_percentage: {rectangle_overlap_percentage} circle_overlap_percentage: {circle_overlap_percentage}\")\n",
    "            rectangles.append(contour)\n",
    "\n",
    "    return circles, rectangles\n",
    "\n",
    "def get_nodes_mask(img_empty_nodes_filled, config):\n",
    "    \"\"\"\n",
    "    Isolates node structures using an iterative erosion/dilation heuristic based on contour count stability.\n",
    "    \"\"\"\n",
    "    # Default values, will be overridden by config if available\n",
    "    erosion_kernel_size = tuple(config.get('shape_detection', {}).get('erosion_kernel_size', [3, 3]))\n",
    "    min_stable_length = config.get('shape_detection', {}).get('min_stable_length', 3)\n",
    "    max_erosion_iterations = config.get('shape_detection', {}).get('max_erosion_iterations', 30)\n",
    "    verbose = config.get('shape_detection', {}).get('verbose', True) # Control printing\n",
    "\n",
    "    erosion_kernel = np.ones(erosion_kernel_size, np.uint8)\n",
    "    contour_counts_history = []\n",
    "    optimal_erosion_iterations = 0  # Default if loop doesn't run or no erosions found necessary\n",
    "    optimal_condition_found = False # Flag to indicate if stability or zero contours was met\n",
    "\n",
    "    # This image is progressively eroded to find the optimal number of iterations\n",
    "    image_for_iterative_erosion = img_empty_nodes_filled.copy()\n",
    "\n",
    "    # Loop to determine the optimal number of erosion iterations\n",
    "    # If max_erosion_iterations is 0, this loop won't execute, and optimal_erosion_iterations will remain 0.\n",
    "    for current_iteration in range(1, max_erosion_iterations + 1):\n",
    "        # Perform one erosion step\n",
    "        eroded_this_step = cv2.erode(image_for_iterative_erosion, erosion_kernel, iterations=1)\n",
    "        contours, _ = cv2.findContours(eroded_this_step, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        num_contours_at_step = len(contours)\n",
    "        contour_counts_history.append(num_contours_at_step)\n",
    "        \n",
    "        image_for_iterative_erosion = eroded_this_step # Update for the next iteration\n",
    "\n",
    "        # Check for stability in contour count\n",
    "        if len(contour_counts_history) >= min_stable_length:\n",
    "            last_n_counts = contour_counts_history[-min_stable_length:]\n",
    "            if all(c == last_n_counts[0] for c in last_n_counts):\n",
    "                # Optimal iterations = iteration count at the start of the stable sequence\n",
    "                optimal_erosion_iterations = current_iteration - min_stable_length + 1\n",
    "                if verbose:\n",
    "                    print(f\"Stability detected: Contour count {last_n_counts[0]} stable for {min_stable_length} iterations.\")\n",
    "                    print(f\"Optimal number of erosions determined as: {optimal_erosion_iterations}.\")\n",
    "                optimal_condition_found = True\n",
    "                break \n",
    "\n",
    "        # Check if all contours have disappeared\n",
    "        if num_contours_at_step == 0:\n",
    "            if not optimal_condition_found: # Only set if stability wasn't the primary reason\n",
    "                optimal_erosion_iterations = current_iteration # All contours gone after this many erosions\n",
    "                if verbose:\n",
    "                    print(f\"All contours disappeared after {current_iteration} erosions.\")\n",
    "                    print(f\"Optimal number of erosions determined as: {optimal_erosion_iterations}.\")\n",
    "            optimal_condition_found = True # This is a definitive condition to stop\n",
    "            break\n",
    "    # Loop ends\n",
    "\n",
    "    # If the loop completed fully (max_erosion_iterations reached) without finding stability or zero contours\n",
    "    if not optimal_condition_found and max_erosion_iterations > 0:\n",
    "        optimal_erosion_iterations = max_erosion_iterations\n",
    "        if verbose:\n",
    "            print(f\"Max erosions ({max_erosion_iterations}) reached without specific stability/zero-contour condition. \"\n",
    "                  f\"Using {optimal_erosion_iterations} erosions.\")\n",
    "\n",
    "    # Obtain the node mask by applying the optimal number of erosions to the original filled image\n",
    "    if optimal_erosion_iterations > 0:\n",
    "        node_mask_eroded = cv2.erode(img_empty_nodes_filled, erosion_kernel, iterations=optimal_erosion_iterations)\n",
    "        if verbose:\n",
    "            print(f\"Applied {optimal_erosion_iterations} erosions to input image to get the node mask.\")\n",
    "    else:\n",
    "        # If no erosions are optimal, return a copy of the input to maintain consistency (always a new image object)\n",
    "        node_mask_eroded = img_empty_nodes_filled.copy()\n",
    "        if verbose:\n",
    "            print(\"No erosions applied for node mask (0 optimal erosions). Using a copy of input.\")\n",
    "\n",
    "    # Dilate the eroded node mask to recover node sizes\n",
    "    if optimal_erosion_iterations > 0:\n",
    "        # Dilate by the same number of steps and with the same kernel\n",
    "        dilated_node_mask = cv2.dilate(node_mask_eroded, erosion_kernel, iterations=optimal_erosion_iterations)\n",
    "        if verbose:\n",
    "            print(f\"Applied {optimal_erosion_iterations} dilations to recover node sizes.\")\n",
    "    else:\n",
    "        # If no erosions were done, no dilations are needed either.\n",
    "        # node_mask_eroded is already a copy of the original (or the optimally eroded one if erosions > 0).\n",
    "        dilated_node_mask = node_mask_eroded \n",
    "        if verbose:\n",
    "            print(\"No dilations applied (0 optimal erosions).\")\n",
    "\n",
    "    if verbose: # Print history only if verbose mode is on\n",
    "        print(f\"Contour counts per erosion iteration: {contour_counts_history}\")\n",
    "\n",
    "    return dilated_node_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56a21b9",
   "metadata": {},
   "source": [
    "### recognize_arrow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "7c66e25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "import cv2\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "\n",
    "# Function to load configuration (can be moved to a central utils.py if used elsewhere)\n",
    "# For now, keep it simple. The main caller (notebook) will load and pass the config.\n",
    "\n",
    "def detect_arrowheads(\n",
    "    image: np.ndarray,\n",
    "    config: dict # Expects the full loaded YAML config\n",
    "    # image_path: str = None # This was unused and can be removed if image is always passed as np.ndarray\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Detects objects (arrowheads) in an image using the Roboflow API.\n",
    "    Configuration for the API (project_id, version, api_key, confidence)\n",
    "    is expected to be in the passed config dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    api_config = config.get('connection_processing', {}).get('arrowhead_api', {})\n",
    "    project_id = api_config.get('project_id')\n",
    "    version = api_config.get('version')\n",
    "    api_key = api_config.get('api_key')\n",
    "    # Roboflow API expects confidence as a percentage (0-100)\n",
    "    confidence = api_config.get('confidence_threshold_percent', 10.0) # Default if not found\n",
    "\n",
    "    if not all([project_id, version, api_key]):\n",
    "        raise ValueError(\"Missing Roboflow API configuration (project_id, version, or api_key) in config.\")\n",
    "    \n",
    "    if api_key == \"YOUR_API_KEY\":\n",
    "        # It's good practice to warn or error if the placeholder API key is still there.\n",
    "        # For now, let's raise an error to prevent accidental calls with a dummy key.\n",
    "        raise ValueError(\"Roboflow API key is set to 'YOUR_API_KEY'. Please update it in your config.yaml.\")\n",
    "\n",
    "    # Encode the image\n",
    "    # The original code had a commented-out section for reading from image_path.\n",
    "    # Sticking to encoding the provided numpy array.\n",
    "    success, encoded_image_bytes = cv2.imencode(\".png\", image) # Using .png as it's lossless; .jpg was also an option\n",
    "    if not success:\n",
    "        raise ValueError(\"Could not encode image to PNG format.\")\n",
    "    \n",
    "    # Base64-encode the image bytes\n",
    "    b64_encoded_image = base64.b64encode(encoded_image_bytes.tobytes()).decode(\"utf-8\")\n",
    "\n",
    "    # Build the request URL with query parameters\n",
    "    # Note: The confidence parameter in the URL is the threshold.\n",
    "    url = (\n",
    "        f\"https://detect.roboflow.com/{project_id}/{version}\"\n",
    "        f\"?api_key={api_key}\"\n",
    "        f\"&confidence={confidence}\"  # This should be the percentage value\n",
    "        \"&format=json\"\n",
    "        # Consider adding other parameters like overlap, stroke, labels if needed,\n",
    "        # and managing them via config.\n",
    "    )\n",
    "\n",
    "    # Send the POST request with the base64-encoded image\n",
    "    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"} # Roboflow expects this for base64 data\n",
    "    response = requests.post(url, data=b64_encoded_image, headers=headers)\n",
    "    response.raise_for_status() # Raises an HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def show_arrows(img, arrowhead_result):\n",
    "    img_drawn = img.copy()\n",
    "    detections = sv.Detections.from_inference(arrowhead_result)\n",
    "\n",
    "    # create supervision annotators\n",
    "    bounding_box_annotator = sv.BoxAnnotator()\n",
    "    label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "    # annotate the image with our inference results\n",
    "    annotated_image = bounding_box_annotator.annotate(\n",
    "        scene=img_drawn, detections=detections)\n",
    "\n",
    "    sv.plot_image(annotated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7674f083",
   "metadata": {},
   "source": [
    "### recognize_arc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "629b5026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "from skimage.morphology import skeletonize\n",
    "import copy \n",
    "\n",
    "def get_hough_lines(img, config):\n",
    "    \"\"\"\n",
    "    Detects lines in the image using Hough Transform.\n",
    "    Returns the detected lines.\n",
    "    \"\"\"\n",
    "    # Default values, will be overridden by config if available\n",
    "    rho = config.get('connection_processing', {}).get('hough_rho', 1)\n",
    "    theta = config.get('connection_processing', {}).get('hough_theta', np.pi / 180)\n",
    "    threshold = config.get('connection_processing', {}).get('hough_threshold', 10)\n",
    "    min_line_length = config.get('connection_processing', {}).get('min_line_length', 10)\n",
    "    max_line_gap = config.get('connection_processing', {}).get('max_line_gap', 20)\n",
    "    min_line_length = max(min_line_length, 1)  # Ensure it's at least 1\n",
    "    \n",
    "\n",
    "        # Skeletonize the image\n",
    "    skeleton = skeletonize(img / 255).astype(np.uint8)*255\n",
    "    hough_lines = cv2.HoughLinesP(skeleton, rho, np.pi/180, threshold, minLineLength=min_line_length, maxLineGap=max_line_gap)\n",
    "    return hough_lines\n",
    "\n",
    "class HoughBundler:     \n",
    "    def __init__(self,min_distance=5,min_angle=2):\n",
    "        self.min_distance = min_distance\n",
    "        self.min_angle = min_angle\n",
    "    \n",
    "    def get_orientation(self, line):\n",
    "        orientation = math.atan2(abs((line[3] - line[1])), abs((line[2] - line[0])))\n",
    "        return math.degrees(orientation)\n",
    "\n",
    "    def check_is_line_different(self, line_1, groups, min_distance_to_merge, min_angle_to_merge):\n",
    "        for group in groups:\n",
    "            for line_2 in group:\n",
    "                if self.get_distance(line_2, line_1) < min_distance_to_merge:\n",
    "                    orientation_1 = self.get_orientation(line_1)\n",
    "                    orientation_2 = self.get_orientation(line_2)\n",
    "                    if abs(orientation_1 - orientation_2) < min_angle_to_merge:\n",
    "                        group.append(line_1)\n",
    "                        return False\n",
    "        return True\n",
    "\n",
    "    def distance_point_to_line(self, point, line):\n",
    "        px, py = point\n",
    "        x1, y1, x2, y2 = line\n",
    "\n",
    "        def line_magnitude(x1, y1, x2, y2):\n",
    "            line_magnitude = math.sqrt(math.pow((x2 - x1), 2) + math.pow((y2 - y1), 2))\n",
    "            return line_magnitude\n",
    "\n",
    "        lmag = line_magnitude(x1, y1, x2, y2)\n",
    "        if lmag < 0.00000001:\n",
    "            distance_point_to_line = 9999\n",
    "            return distance_point_to_line\n",
    "\n",
    "        u1 = (((px - x1) * (x2 - x1)) + ((py - y1) * (y2 - y1)))\n",
    "        u = u1 / (lmag * lmag)\n",
    "\n",
    "        if (u < 0.00001) or (u > 1):\n",
    "            #// closest point does not fall within the line segment, take the shorter distance\n",
    "            #// to an endpoint\n",
    "            ix = line_magnitude(px, py, x1, y1)\n",
    "            iy = line_magnitude(px, py, x2, y2)\n",
    "            if ix > iy:\n",
    "                distance_point_to_line = iy\n",
    "            else:\n",
    "                distance_point_to_line = ix\n",
    "        else:\n",
    "            # Intersecting point is on the line, use the formula\n",
    "            ix = x1 + u * (x2 - x1)\n",
    "            iy = y1 + u * (y2 - y1)\n",
    "            distance_point_to_line = line_magnitude(px, py, ix, iy)\n",
    "\n",
    "        return distance_point_to_line\n",
    "\n",
    "    def get_distance(self, a_line, b_line):\n",
    "        dist1 = self.distance_point_to_line(a_line[:2], b_line)\n",
    "        dist2 = self.distance_point_to_line(a_line[2:], b_line)\n",
    "        dist3 = self.distance_point_to_line(b_line[:2], a_line)\n",
    "        dist4 = self.distance_point_to_line(b_line[2:], a_line)\n",
    "\n",
    "        return min(dist1, dist2, dist3, dist4)\n",
    "\n",
    "    def merge_lines_into_groups(self, lines):\n",
    "        groups = []  # all lines groups are here\n",
    "        # first line will create new group every time\n",
    "        groups.append([lines[0]])\n",
    "        # if line is different from existing gropus, create a new group\n",
    "        for line_new in lines[1:]:\n",
    "            if self.check_is_line_different(line_new, groups, self.min_distance, self.min_angle):\n",
    "                groups.append([line_new])\n",
    "\n",
    "        return groups\n",
    "\n",
    "    def merge_line_segments(self, lines):\n",
    "        orientation = self.get_orientation(lines[0])\n",
    "      \n",
    "        if(len(lines) == 1):\n",
    "            return np.block([[lines[0][:2], lines[0][2:]]])\n",
    "\n",
    "        points = []\n",
    "        for line in lines:\n",
    "            points.append(line[:2])\n",
    "            points.append(line[2:])\n",
    "        if 45 < orientation <= 90:\n",
    "            #sort by y\n",
    "            points = sorted(points, key=lambda point: point[1])\n",
    "        else:\n",
    "            #sort by x\n",
    "            points = sorted(points, key=lambda point: point[0])\n",
    "\n",
    "        return np.block([[points[0],points[-1]]])\n",
    "\n",
    "    def process_lines(self, lines):\n",
    "        lines_horizontal  = []\n",
    "        lines_vertical  = []\n",
    "  \n",
    "        for line_i in [l[0] for l in lines]:\n",
    "            orientation = self.get_orientation(line_i)\n",
    "            # if vertical\n",
    "            if 45 < orientation <= 90:\n",
    "                lines_vertical.append(line_i)\n",
    "            else:\n",
    "                lines_horizontal.append(line_i)\n",
    "\n",
    "        lines_vertical  = sorted(lines_vertical , key=lambda line: line[1])\n",
    "        lines_horizontal  = sorted(lines_horizontal , key=lambda line: line[0])\n",
    "        merged_lines_all = []\n",
    "\n",
    "        # for each cluster in vertical and horizantal lines leave only one line\n",
    "        for i in [lines_horizontal, lines_vertical]:\n",
    "            if len(i) > 0:\n",
    "                groups = self.merge_lines_into_groups(i)\n",
    "                merged_lines = []\n",
    "                for group in groups:\n",
    "                    merged_lines.append(self.merge_line_segments(group))\n",
    "                merged_lines_all.extend(merged_lines)\n",
    "                    \n",
    "        return np.asarray(merged_lines_all)\n",
    "    \n",
    "def assign_proximity_nodes(\n",
    "    original_lines: list[Line], \n",
    "    original_places: list[Place], \n",
    "    original_transitions: list[Transition], \n",
    "    config: dict,\n",
    ") -> tuple[list[Line], list[Place], list[Transition]]:\n",
    "    \"\"\"\n",
    "    Assigns a 'proximity_node' attribute to points of lines if they are close\n",
    "    to a place or transition. Operates on deep copies of the input objects.\n",
    "\n",
    "    Args:\n",
    "        original_lines: A list of Line objects.\n",
    "        original_places: A list of Place objects.\n",
    "        original_transitions: A list of Transition objects.\n",
    "        proximity_threshold: A factor to expand node boundaries for proximity checks.\n",
    "                           For Places, it scales the radius.\n",
    "                           For Transitions, it scales the height and width.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - lines_copy: Copied lines, where points may have a 'proximity_node' attribute.\n",
    "        - places_copy: Deep copies of the original places.\n",
    "        - transitions_copy: Deep copies of the original transitions.\n",
    "        The 'proximity_node' attributes will refer to objects within places_copy or transitions_copy.\n",
    "    \"\"\"\n",
    "    proximity_thres_place = config.get('connection_processing', {}).get('proximity_thres_place', 1.5)\n",
    "    proximity_thres_trans_width = config.get('connection_processing', {}).get('proximity_thres_trans_width', 3)\n",
    "    proximity_thres_trans_height = config.get('connection_processing', {}).get('proximity_thres_trans_height', 1.2)\n",
    "\n",
    "    # 1. Create deep copies of all input object lists\n",
    "    lines_copy = copy.deepcopy(original_lines)\n",
    "    places_copy = copy.deepcopy(original_places)\n",
    "    transitions_copy = copy.deepcopy(original_transitions)\n",
    "\n",
    "    all_copied_node_centers = [node.center for node in places_copy] + \\\n",
    "                              [node.center for node in transitions_copy]\n",
    "\n",
    "    # 3. Iterate through copied lines and their points\n",
    "    for line in lines_copy:\n",
    "        for line_point in [line.point1, line.point2]:\n",
    "            for node_center_copy in all_copied_node_centers:\n",
    "                node_copy = node_center_copy.part_of \n",
    "                # print(f\"Checking proximity for line point {line_point} to node {node_copy}\")\n",
    "\n",
    "                if isinstance(node_copy, Place):\n",
    "                    # print(f\"Node {node_copy} is a place\")\n",
    "                    distance = line_point.get_distance_between_points(node_center_copy)\n",
    "                    if distance < proximity_thres_place * node_copy.radius:\n",
    "                        line_point.proximity_node = node_copy\n",
    "                \n",
    "                elif isinstance(node_copy, Transition):\n",
    "                    # print(f\"Node {node_copy} is a transition\")\n",
    "                    expanded_height = node_copy.height * proximity_thres_trans_height\n",
    "                    expanded_width = node_copy.width * proximity_thres_trans_width\n",
    "                    \n",
    "                    \n",
    "                    expanded_bbox_contour = cv2.boxPoints(((float(node_center_copy.x), float(node_center_copy.y)),\n",
    "                                                            (expanded_height, expanded_width), node_copy.angle))\n",
    "                    current_line_point_coords = (float(line_point.x), float(line_point.y))\n",
    "                    \n",
    "                    if cv2.pointPolygonTest(expanded_bbox_contour, current_line_point_coords, False) >= 0:\n",
    "                        line_point.proximity_node = node_copy\n",
    "\n",
    "                else:\n",
    "                    print(f\"Node {node_copy} is not a recognized type\")\n",
    "                    # This case should ideally not be reached if inputs are as expected.\n",
    "                    raise ValueError(f\"Unknown node type encountered: {type(node_copy)}\")\n",
    "    \n",
    "    return lines_copy, places_copy, transitions_copy\n",
    "\n",
    "def get_entry_points_from_lines(lines_list):\n",
    "    \"\"\"\n",
    "    Original function provided by user, slightly adapted to use a local list.\n",
    "    Extracts all unique points marked as 'is_entry' from a list of lines.\n",
    "    \"\"\"\n",
    "    entry_points_set = set()\n",
    "    for line in lines_list:\n",
    "        if hasattr(line.point1, \"proximity_node\") and line.point1.proximity_node:\n",
    "            entry_points_set.add(line.point1)\n",
    "        if hasattr(line.point2, \"proximity_node\") and line.point2.proximity_node:\n",
    "            entry_points_set.add(line.point2)\n",
    "    return list(entry_points_set)\n",
    "\n",
    "\n",
    "def cosine_similarity(vec1_norm: np.ndarray, vec2_norm: np.ndarray) -> float:\n",
    "    \"\"\"Computes the cosine similarity (dot product of normalized vectors).\"\"\"\n",
    "    return np.dot(vec1_norm, vec2_norm)\n",
    "\n",
    "def find_line_paths(\n",
    "    initial_lines_list: list[Line],\n",
    "    proximity_threshold: float = 30.0,\n",
    "    dot_product_weight: float = 0.6,\n",
    "    distance_to_line_weight: float = 0.2,\n",
    "    endpoint_distance_weight: float = 0.2\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Connects lines from a pool into paths, starting from an entry point\n",
    "    and ending at another entry point.\n",
    "\n",
    "    Args:\n",
    "        initial_lines_list: A list of Line objects.\n",
    "        proximity_threshold: Maximum distance to search for next point.\n",
    "        dot_product_weight: Weight for vector alignment score.\n",
    "        distance_to_line_weight: Weight for point-to-line distance score.\n",
    "        endpoint_distance_weight: Weight for endpoint-to-endpoint distance score.\n",
    "\n",
    "    Returns:\n",
    "        A list of paths. Each path is a dictionary with 'lines' (list of Line)\n",
    "        and 'points' (ordered list of Point forming the path).\n",
    "    \"\"\"\n",
    "    lines_pool = set(initial_lines_list) # Use a set for efficient removal (O(1) on average)\n",
    "    all_paths_found = []\n",
    "    \n",
    "    # Keep track of entry points that have successfully started a path to avoid re-processing\n",
    "    # or entry points that have been used as an end of a path.\n",
    "    consumed_entry_points = set()\n",
    "\n",
    "    while True:\n",
    "        current_start_line = None\n",
    "        current_start_entry_point = None\n",
    "\n",
    "        # Find a new starting line with an available entry point\n",
    "        # Iterate over a temporary list as lines_pool can be modified\n",
    "        for line in list(lines_pool):\n",
    "            potential_start_points = []\n",
    "            if hasattr(line.point1, \"proximity_node\") and line.point1.proximity_node and line.point1 not in consumed_entry_points:\n",
    "                potential_start_points.append(line.point1)\n",
    "            if hasattr(line.point2, \"proximity_node\") and line.point2.proximity_node and line.point2 not in consumed_entry_points:\n",
    "                potential_start_points.append(line.point2)\n",
    "            \n",
    "            if potential_start_points:\n",
    "                current_start_line = line\n",
    "                # Prefer point1 if both are entries and available, or just take the first one.\n",
    "                current_start_entry_point = potential_start_points[0]\n",
    "                break\n",
    "        \n",
    "        if not current_start_line:\n",
    "            break # No more available entry points or lines to start a path\n",
    "\n",
    "        current_path_lines = [current_start_line]\n",
    "        current_path_points = [current_start_entry_point]\n",
    "        \n",
    "        lines_pool.remove(current_start_line)\n",
    "        consumed_entry_points.add(current_start_entry_point) # Mark this entry point as used for path initiation\n",
    "\n",
    "        last_line_in_path = current_start_line\n",
    "        # The current tip of the path is the other point of the start_line\n",
    "        current_tip_of_path = last_line_in_path.get_other_point(current_start_entry_point)\n",
    "        current_path_points.append(current_tip_of_path)\n",
    "\n",
    "        # Inner loop to extend the current path\n",
    "        while True:\n",
    "            # Check if the current_tip_of_path is a destination entry point\n",
    "            if hasattr(current_tip_of_path, \"proximity_node\") and current_tip_of_path.proximity_node:\n",
    "                all_paths_found.append({\"lines\": list(current_path_lines), \"points\": list(current_path_points)})\n",
    "                consumed_entry_points.add(current_tip_of_path) # Mark end entry point\n",
    "                break # Path successfully found, break from inner loop\n",
    "\n",
    "            candidate_extensions = []\n",
    "            # Vector of the last segment, oriented towards the current tip\n",
    "            vec_last_segment_norm = last_line_in_path.get_normalized_vector(\n",
    "                start_point=last_line_in_path.get_other_point(current_tip_of_path),\n",
    "                end_point=current_tip_of_path\n",
    "            )\n",
    "\n",
    "            for candidate_line in list(lines_pool): # Iterate over a copy of the pool for safe removal\n",
    "                for point_on_candidate in [candidate_line.point1, candidate_line.point2]:\n",
    "                    # Must not connect via an intermediate entry point\n",
    "                    if hasattr(point_on_candidate, \"proximity_node\") and point_on_candidate.proximity_node:\n",
    "                        continue\n",
    "\n",
    "                    endpoint_dist = current_tip_of_path.get_distance_between_points(point_on_candidate)\n",
    "\n",
    "                    if endpoint_dist <= proximity_threshold:\n",
    "                        # Scoring Criterion 1: Dot product of normalized vectors\n",
    "                        # Vector of candidate_line, oriented away from point_on_candidate\n",
    "                        vec_candidate_norm = candidate_line.get_normalized_vector(\n",
    "                            start_point=point_on_candidate,\n",
    "                            end_point=candidate_line.get_other_point(point_on_candidate)\n",
    "                        )\n",
    "                        dot_prod_score = (cosine_similarity(vec_last_segment_norm, vec_candidate_norm) + 1) / 2 # Scale to [0,1]\n",
    "\n",
    "                        # Scoring Criterion 2: Start point of \"to be merged\" line is close to the infinite line\n",
    "                        # formed by our last merged line.\n",
    "                        dist_to_prev_line_inf = last_line_in_path.distance_point_to_infinite_line(point_on_candidate)\n",
    "                        # Score: higher is better (closer to 0 distance)\n",
    "                        # Avoid division by zero; add 1. Max possible distance could normalize this.\n",
    "                        # For simplicity, using 1 / (1 + dist).\n",
    "                        dist_line_score = 1.0 / (1.0 + dist_to_prev_line_inf) if proximity_threshold > 0 else 1.0\n",
    "\n",
    "\n",
    "                        # Bonus: endpoint_distance score (closer is better)\n",
    "                        endpoint_dist_score = (proximity_threshold - endpoint_dist) / proximity_threshold \\\n",
    "                                              if proximity_threshold > 0 else 1.0\n",
    "                        \n",
    "                        # Combined score\n",
    "                        total_score = (dot_product_weight * dot_prod_score +\n",
    "                                       distance_to_line_weight * dist_line_score +\n",
    "                                       endpoint_distance_weight * endpoint_dist_score)\n",
    "                        \n",
    "                        candidate_extensions.append({\n",
    "                            \"line\": candidate_line,\n",
    "                            \"connection_point_on_candidate\": point_on_candidate,\n",
    "                            \"score\": total_score\n",
    "                        })\n",
    "            \n",
    "            if not candidate_extensions:\n",
    "                # No suitable extension found, path terminates here (not at an entry point).\n",
    "                # This path is considered \"noise\" or incomplete.\n",
    "                break # Break from inner loop\n",
    "\n",
    "            # Select the best candidate extension\n",
    "            candidate_extensions.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "            best_extension = candidate_extensions[0]\n",
    "\n",
    "            # Add best extension to the current path\n",
    "            lines_pool.remove(best_extension[\"line\"]) # Remove from available lines\n",
    "            current_path_lines.append(best_extension[\"line\"])\n",
    "            \n",
    "            last_line_in_path = best_extension[\"line\"]\n",
    "            # The connection point on the candidate becomes part of the path\n",
    "            current_path_points.append(best_extension[\"connection_point_on_candidate\"])\n",
    "            # The new tip is the other end of the newly added line\n",
    "            current_tip_of_path = last_line_in_path.get_other_point(best_extension[\"connection_point_on_candidate\"])\n",
    "            current_path_points.append(current_tip_of_path)\n",
    "            # Continue extending this path\n",
    "\n",
    "    return all_paths_found\n",
    "\n",
    "def assign_arrowheads(found_paths_original: list[Line], arrowhead_result: dict, config) -> tuple[list[Line], int]:\n",
    "    arrowhead_proximity_thres = config.get('connection_processing', {}).get('arrowhead_proximity_threshold', 30)\n",
    "\n",
    "    found_paths_copy = copy.deepcopy(found_paths_original)\n",
    "    path_endpoints = []\n",
    "    for path in found_paths_copy:\n",
    "        if path[\"points\"]: \n",
    "            path_endpoints.extend([path[\"points\"][0], path[\"points\"][-1]])\n",
    "        else:\n",
    "            raise ValueError(\"Path points list is empty. Cannot assign arrowheads.\")\n",
    "        \n",
    "    rejected_arrowhead_count = 0\n",
    "\n",
    "    for arrowhead in arrowhead_result[\"predictions\"]:\n",
    "        arrowhead_center = Point(arrowhead[\"x\"], arrowhead[\"y\"])\n",
    "        \n",
    "        closest_point = None\n",
    "        closest_distance = float(\"inf\")\n",
    "        for endpoint in path_endpoints:\n",
    "            distance = arrowhead_center.get_distance_between_points(endpoint)\n",
    "            if distance < closest_distance and distance < arrowhead_proximity_thres:\n",
    "                closest_distance = distance\n",
    "                closest_point = endpoint\n",
    "                ### remove endpoint from endpoints to avoid reusing\n",
    "                path_endpoints.remove(endpoint)\n",
    "        \n",
    "        ### check if the closest point is None and throw error\n",
    "        if closest_point is None:\n",
    "            print(\"No closest point found for the arrowhead center.\")\n",
    "            rejected_arrowhead_count += 1\n",
    "        else: \n",
    "            closest_point.is_arrow = True\n",
    "\n",
    "    return found_paths_copy, rejected_arrowhead_count\n",
    "\n",
    "def get_arcs(paths):\n",
    "    \"\"\"\n",
    "    Links the nodes of the paths based on the proximity_node attribute of the points.\n",
    "    This function assumes that the paths are already processed and contain points with proximity_node.\n",
    "    \"\"\"\n",
    "\n",
    "    arcs = []\n",
    "\n",
    "    for path in paths:\n",
    "        if not path[\"points\"][0].proximity_node or not path[\"points\"][-1].proximity_node:\n",
    "            raise ValueError(\"Path must start and end with a proximity node.\")\n",
    "        if len(path[\"points\"]) < 2:\n",
    "            raise ValueError(\"Path must contain at least two points.\")\n",
    "        if len(path[\"lines\"]) < 1:\n",
    "            raise ValueError(\"Path must contain at least one line.\")\n",
    "        # Assuming a path of N points connected sequentially has N-1 lines.\n",
    "        if len(path[\"points\"]) != len(path[\"lines\"]) * 2:\n",
    "             raise ValueError(\"Path points and lines are inconsistent.\")\n",
    "\n",
    "        start_point = path[\"points\"][0]\n",
    "        end_point = path[\"points\"][-1]\n",
    "\n",
    "        # Add arc from start to end unless start is an arrow and end is not\n",
    "        if not (start_point.is_arrow and not end_point.is_arrow):\n",
    "            arcs.append(Arc(\n",
    "                source=start_point.proximity_node,\n",
    "                target=end_point.proximity_node,\n",
    "                start_point=start_point,\n",
    "                end_point=end_point,\n",
    "                points=path[\"points\"],\n",
    "                lines=path[\"lines\"]\n",
    "            ))\n",
    "\n",
    "        # Add arc from end to start if start is an arrow\n",
    "        if start_point.is_arrow:\n",
    "             arcs.append(Arc(\n",
    "                source=end_point.proximity_node,\n",
    "                target=start_point.proximity_node,\n",
    "                start_point=end_point,\n",
    "                end_point=start_point,\n",
    "                points=path[\"points\"],\n",
    "                lines=path[\"lines\"]\n",
    "            ))\n",
    "\n",
    "    return arcs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cfb3f7",
   "metadata": {},
   "source": [
    "### data_loading.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "3244a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess(img):\n",
    "    \"\"\"Applies Otsu's thresholding to the input image.\"\"\"\n",
    "    # Ensure input is grayscale if it's not already\n",
    "    gray_img = img\n",
    "    if len(img.shape) == 3 and img.shape[2] == 3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    _, thresh_otsu = cv2.threshold(gray_img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    return thresh_otsu\n",
    "\n",
    "def load_and_preprocess_image(image_path: str, config: dict):\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"Image file not found at: {image_path}\")\n",
    "\n",
    "    img_color = cv2.imread(image_path)\n",
    "    if img_color is None:\n",
    "        raise ValueError(f\"Could not read image file: {image_path}\")\n",
    "\n",
    "    img_gray = cv2.cvtColor(img_color, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # --- Upscaling Heuristic ---\n",
    "    cfg_proc = config.get('image_processing', {})\n",
    "    min_dimension_threshold = cfg_proc.get('min_dimension_threshold', 800)\n",
    "    upscale_factor = cfg_proc.get('upscale_factor', 2)\n",
    "\n",
    "    h, w = img_gray.shape\n",
    "    img_color_resized = img_color\n",
    "    img_gray_resized = img_gray\n",
    "\n",
    "    if h < min_dimension_threshold or w < min_dimension_threshold:\n",
    "        print(f\"Image dimensions ({w}x{h}) below threshold ({min_dimension_threshold}px). Upscaling by {upscale_factor}x.\")\n",
    "        new_w, new_h = w * upscale_factor, h * upscale_factor\n",
    "        img_gray_resized = cv2.resize(img_gray, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)\n",
    "        img_color_resized = cv2.resize(img_color, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)\n",
    "\n",
    "    # --- Initial Preprocessing (Inversion + Thresholding) ---\n",
    "    img_inverted = cv2.bitwise_not(img_gray_resized)\n",
    "    preprocessed_img = preprocess(img_inverted) \n",
    "\n",
    "    return preprocessed_img, img_color_resized, img_gray_resized "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6684c2cc",
   "metadata": {},
   "source": [
    "### models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "d3db72fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import largestinteriorrectangle as lir\n",
    "import cv2\n",
    "\n",
    "# Epsilon for floating point comparisons if needed, though not used in current definitions\n",
    "EPS = 1e-6 \n",
    "\n",
    "class Point:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = int(x) # Ensure integer coordinates if they represent pixels\n",
    "        self.y = int(y)\n",
    "\n",
    "        self.proximity_node = None # Placeholder for proximity node assignment\n",
    "        self.is_arrow = False # Placeholder for entry point assignment\n",
    "\n",
    "    def get_distance_between_points(self, other_point):\n",
    "        \"\"\"Calculate Euclidean distance between this point and another point.\"\"\"\n",
    "        return math.sqrt((self.x - other_point.x) ** 2 + (self.y - other_point.y) ** 2)\n",
    "    \n",
    "    def is_inside_contour(self, contour):\n",
    "        \"\"\"Check if this point is inside a given contour using cv2.pointPolygonTest\"\"\"\n",
    "        # Note: This requires cv2, which might be better placed in a different module\n",
    "        point_tuple = (float(self.x), float(self.y)) # pointPolygonTest needs float tuple\n",
    "        # Ensure contour is in the correct format (e.g., Nx1x2 or Nx2)\n",
    "        try:\n",
    "            # >= 0 means inside or on the boundary\n",
    "            return cv2.pointPolygonTest(contour, point_tuple, False) >= 0 \n",
    "        except NameError:\n",
    "            print(\"Warning: cv2 not imported. is_inside_contour cannot function.\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"Error during pointPolygonTest: {e}\")\n",
    "            return False\n",
    "        \n",
    "    def get_numpy_array(self):\n",
    "        \"\"\"Returns the point as a numpy array.\"\"\"\n",
    "        return np.array([self.x, self.y], dtype=np.int32)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Point({self.x}, {self.y})\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, Point):\n",
    "            return NotImplemented\n",
    "        return self.x == other.x and self.y == other.y\n",
    "\n",
    "    def __hash__(self):\n",
    "        \"\"\"Allows Point objects to be added to sets or used as dictionary keys.\"\"\"\n",
    "        return hash((self.x, self.y))\n",
    "\n",
    "class Line:\n",
    "    def __init__(self, start_point: Point, end_point: Point, angle=None, length=None):\n",
    "        \"\"\"\n",
    "        Initializes a Line object.\n",
    "        If angle and length are not provided, they are calculated.\n",
    "        \"\"\"\n",
    "        self.point1 = start_point\n",
    "        self.point2 = end_point\n",
    "\n",
    "        # Assign self to the points for back-reference if needed later\n",
    "        # self.point1.part_of = self \n",
    "        # self.point2.part_of = self\n",
    "\n",
    "        if angle is None or length is None:\n",
    "            dx = self.point2.x - self.point1.x\n",
    "            dy = self.point2.y - self.point1.y\n",
    "            # Calculate angle in degrees\n",
    "            self.angle = math.degrees(math.atan2(dy, dx)) if not (dx == 0 and dy == 0) else 0.0\n",
    "            # Calculate length\n",
    "            self.length = self.point1.get_distance_between_points(self.point2)\n",
    "        else:\n",
    "            self.angle = angle\n",
    "            self.length = length\n",
    "\n",
    "    def get_other_point(self, point: Point) -> Point:\n",
    "        \"\"\"Given one point of the line, returns the other point.\"\"\"\n",
    "        if point == self.point1:\n",
    "            return self.point2\n",
    "        elif point == self.point2:\n",
    "            return self.point1\n",
    "        else:\n",
    "            # This case should ideally not be reached if logic is correct\n",
    "            raise ValueError(\"Point is not part of this line.\")\n",
    "\n",
    "    def get_vector(self, start_point: Point = None, end_point: Point = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns the vector of the line.\n",
    "        If start_point and end_point are provided, computes vector from start to end.\n",
    "        Otherwise, defaults to point1 -> point2.\n",
    "        \"\"\"\n",
    "        if start_point and end_point:\n",
    "            return np.array([end_point.x - start_point.x, end_point.y - start_point.y])\n",
    "        return np.array([self.point2.x - self.point1.x, self.point2.y - self.point1.y])\n",
    "\n",
    "    def get_normalized_vector(self, start_point: Point = None, end_point: Point = None) -> np.ndarray:\n",
    "        \"\"\"Returns the normalized (unit) vector of the line.\"\"\"\n",
    "        vec = self.get_vector(start_point, end_point)\n",
    "        norm = np.linalg.norm(vec)\n",
    "        if norm == 0:\n",
    "            return np.array([0, 0]) # Represents a zero-length line segment\n",
    "        return vec / norm\n",
    "\n",
    "    def distance_point_to_infinite_line(self, point: Point) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the perpendicular distance from a point to the infinite line\n",
    "        defined by this line segment.\n",
    "        \"\"\"\n",
    "        p1_np = np.array([self.point1.x, self.point1.y])\n",
    "        p2_np = np.array([self.point2.x, self.point2.y])\n",
    "        p3_np = np.array([point.x, point.y])\n",
    "\n",
    "        if np.array_equal(p1_np, p2_np): # If the line is just a point\n",
    "            return np.linalg.norm(p3_np - p1_np)\n",
    "\n",
    "        numerator = np.abs(np.cross(p2_np - p1_np, p1_np - p3_np))\n",
    "        denominator = np.linalg.norm(p2_np - p1_np)\n",
    "        if denominator == 0:\n",
    "            return np.linalg.norm(p3_np - p1_np) # Distance to the single point\n",
    "        return numerator / denominator\n",
    "    \n",
    "    def distance_point_to_segment(self, point: Point) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the shortest distance from a query point to this line segment.\n",
    "        \"\"\"\n",
    "        # Convert query point and segment endpoints to numpy arrays\n",
    "        p_np = point.get_numpy_array().astype(float)\n",
    "        a_np = self.point1.get_numpy_array().astype(float) # Segment start (self.point1)\n",
    "        b_np = self.point2.get_numpy_array().astype(float) # Segment end (self.point2)\n",
    "\n",
    "        # If the segment is essentially a point (point1 and point2 are the same)\n",
    "        if self.point1 == self.point2: # Relies on Point.__eq__\n",
    "            return point.get_distance_between_points(self.point1)\n",
    "\n",
    "        # Vector from A to B (segment vector)\n",
    "        vec_ab = b_np - a_np\n",
    "        # Vector from A to P (point relative to segment start)\n",
    "        vec_ap = p_np - a_np\n",
    "\n",
    "        t = np.dot(vec_ap, vec_ab) / np.dot(vec_ab, vec_ab)\n",
    "\n",
    "        if 0.0 <= t <= 1.0:\n",
    "            # The projection falls on the segment AB.\n",
    "            # The shortest distance is the perpendicular distance from P to the line AB.\n",
    "            # This can be calculated by self.distance_point_to_infinite_line(point).\n",
    "            return self.distance_point_to_infinite_line(point)\n",
    "        elif t < 0.0:\n",
    "            # The projection falls outside the segment, on the side of A.\n",
    "            # The closest point on the segment to P is A (self.point1).\n",
    "            return point.get_distance_between_points(self.point1)\n",
    "        else: # t > 1.0\n",
    "            return point.get_distance_between_points(self.point2)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Line(start={self.point1}, end={self.point2}, angle={self.angle:.2f}, length={self.length:.2f})\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, Line):\n",
    "            return NotImplemented\n",
    "        # A line is considered equal if its endpoints are the same, regardless of order.\n",
    "        return (self.point1 == other.point1 and self.point2 == other.point2) or \\\n",
    "               (self.point1 == other.point2 and self.point2 == other.point1)\n",
    "\n",
    "    def __hash__(self):\n",
    "        \"\"\"Allows Line objects to be added to sets. The hash is order-invariant for points.\"\"\"\n",
    "        # Hash the tuple of sorted point hashes\n",
    "        return hash(tuple(sorted((hash(self.point1), hash(self.point2)))))\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "#####################################################################\n",
    "class Place:\n",
    "    def __init__(\n",
    "        self,\n",
    "        circle: tuple[int, int, int], # (x, y, radius)\n",
    "        original_detection_data=None, # Placeholder for any original detection data\n",
    "    ):\n",
    "        self.center = Point(circle[0], circle[1])\n",
    "        self.radius = circle[2]\n",
    "        self.center.part_of = self # Link back to the Place object\n",
    "\n",
    "        self.text = [] # Placeholder for any text associated with this place\n",
    "        self.original_detection_data = original_detection_data \n",
    "\n",
    "        self.markers = 0 # Placeholder for markers associated with this place\n",
    "\n",
    "    @classmethod\n",
    "    def from_contour(cls, contour: np.ndarray):\n",
    "        (x, y), radius = cv2.minEnclosingCircle(contour)\n",
    "        return cls((x, y, radius), original_detection_data= contour)\n",
    "    \n",
    "    def update_markers_from_text(self):\n",
    "        \"\"\"\n",
    "        Recalculates and updates self.markers by summing numeric values\n",
    "        from associated Text objects in self.text.\n",
    "        Only text values that consist purely of digits after stripping whitespace\n",
    "        are considered numeric.\n",
    "        \"\"\"\n",
    "        current_sum_of_markers = 0\n",
    "        for text_obj in self.text: # self.text is a list of Text objects\n",
    "            value_str = text_obj.value.strip()\n",
    "            if is_number(value_str):\n",
    "                try:\n",
    "                    num_val = float(value_str)\n",
    "                    # Check for infinity, as int(inf) raises OverflowError\n",
    "                    if num_val != float('inf') and num_val != float('-inf'):\n",
    "                        current_sum_of_markers += int(num_val)\n",
    "                    # else: print(f\"Info: Skipped infinite value '{value_str}' for markers.\") # Optional logging\n",
    "                except ValueError:\n",
    "                    pass \n",
    "        self.markers = current_sum_of_markers\n",
    "        \n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Place(center={self.center}, radius={self.radius})\"\n",
    "\n",
    "class Transition:\n",
    "    def __init__(\n",
    "        self,\n",
    "        center_coords: tuple[int, int], # (x, y)\n",
    "        height: int,\n",
    "        width: int,\n",
    "        angle: float = 0.0, # Default angle\n",
    "        original_detection_data=None, \n",
    "    ):\n",
    "        self.center = Point(center_coords[0], center_coords[1])\n",
    "        self.center.part_of = self\n",
    "\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.angle = angle # Angle in degrees\n",
    "\n",
    "        self.box_points = cv2.boxPoints(((self.center.x, self.center.y), (self.height, self.width), angle))\n",
    "\n",
    "        self.points = [Point(int(pt[0]), int(pt[1])) for pt in self.box_points]\n",
    "        for point in self.points:\n",
    "            point.part_of = self\n",
    "\n",
    "        self.text = [] \n",
    "\n",
    "        self.original_detection_data = original_detection_data \n",
    "\n",
    "    @classmethod\n",
    "    def from_contour(cls, contour: np.ndarray):\n",
    "        min_area_rect = cv2.minAreaRect(contour)\n",
    "        return cls(min_area_rect[0], min_area_rect[1][0], min_area_rect[1][1], min_area_rect[2], original_detection_data=contour)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Transition(center={self.center}, height={self.height}, width={self.width}, angle={self.angle})\"\n",
    "\n",
    "### Potentially add an Arc class later if needed to represent the final connections\n",
    "class Arc:\n",
    "    def __init__(self, source, target, start_point, end_point, points=None, lines=None):\n",
    "        self.source = source # Place or Transition object\n",
    "        self.target = target # Place or Transition object\n",
    "        self.start_point = start_point # Point object\n",
    "        self.end_point = end_point # Point object\n",
    "        self.points = points # Optional: Ordered list of points forming the arc geometry\n",
    "        self.lines = lines   # Optional: List of Line segments forming the arc geometry\n",
    "\n",
    "        self.text = [] # Placeholder for any text associated with this place\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Arc(source={self.source}, target={self.target})\"\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, Arc):\n",
    "            return NotImplemented\n",
    "        return (self.source == other.source and self.target == other.target)\n",
    "\n",
    "class Text:\n",
    "    \"\"\"Represents a detected text element with its content and bounding box.\"\"\"\n",
    "    # Store geometry as absolute integer coordinates\n",
    "    def __init__(self, value: str, geometry_abs: tuple[tuple[int, int], tuple[int, int]], confidence: float):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            value: The recognized text string.\n",
    "            geometry_abs: Bounding box absolute coordinates ((xmin, ymin), (xmax, ymax)).\n",
    "            confidence: The recognition confidence score.\n",
    "        \"\"\"\n",
    "        self.value = value\n",
    "        self.pt1 = Point(geometry_abs[0][0], geometry_abs[0][1])\n",
    "        self.pt2 = Point(geometry_abs[1][0], geometry_abs[1][1])\n",
    "        self.center = Point(\n",
    "            (self.pt1.x + self.pt2.x) // 2,\n",
    "            (self.pt1.y + self.pt2.y) // 2\n",
    "        )\n",
    "        self.confidence = confidence\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Text(value='{self.value}', box=({self.pt1.x},{self.pt1.y})-({self.pt2.x},{self.pt2.y}), conf={self.confidence:.2f})\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f65b6ae",
   "metadata": {},
   "source": [
    "### workflow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "1490d25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded from config.yaml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from PIL import Image\n",
    "\n",
    "# ## 2. Configuration Loading\n",
    "CONFIG_PATH = 'config.yaml'\n",
    "config = {}\n",
    "\n",
    "try:\n",
    "    with open(CONFIG_PATH, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(f\"Configuration loaded from {CONFIG_PATH}\") # Kept simple confirmation\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {CONFIG_PATH} not found. Using empty config.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or parsing {CONFIG_PATH}: {e}. Using empty config.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "55984fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image dimensions (1057x619) below threshold (800px). Upscaling by 2x.\n",
      "Image loaded and preprocessed from: ../data/local/mid_petri_2.png\n"
     ]
    }
   ],
   "source": [
    "INPUT_IMAGE_PATH = '../data/local/mid_petri_2.png' # Example relative path\n",
    "# INPUT_IMAGE_PATH = '../data/internet/petri_net_19.jpg' # rect thresh 0.85\n",
    "# INPUT_IMAGE_PATH = '../data/internet/petri_net_12.jpg' # rect thresh 0.85\n",
    "\n",
    "img_steps = []\n",
    "\n",
    "preprocessed_img, img_color_resized, img_gray_resized = load_and_preprocess_image(INPUT_IMAGE_PATH, config)\n",
    "print(f\"Image loaded and preprocessed from: {INPUT_IMAGE_PATH}\") # Kept simple confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "999ecb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_steps.append(Image.fromarray(img_color_resized))\n",
    "Image.fromarray(img_color_resized).show(title=\"Image without shapes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "418b0630",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_text_list = detect_text(img_color_resized, config)\n",
    "img_no_text = get_img_no_text(preprocessed_img, detected_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "1537c2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(img_no_text).show(title=\"Image without text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "715a523b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stability detected: Contour count 31 stable for 3 iterations.\n",
      "Optimal number of erosions determined as: 6.\n",
      "Applied 6 erosions to input image to get the node mask.\n",
      "Applied 6 dilations to recover node sizes.\n",
      "Contour counts per erosion iteration: [936, 72, 69, 69, 42, 31, 31, 31]\n"
     ]
    }
   ],
   "source": [
    "circles, rectangles = detect_shapes(img_no_text, config)\n",
    "img_empty_nodes_filled = fill_contours(img_no_text, circles + rectangles)\n",
    "\n",
    "nodes_mask = get_nodes_mask(img_empty_nodes_filled, config) \n",
    "# Image.fromarray(nodes_mask).show(title=\"Isolated Nodes Mask\")\n",
    "\n",
    "detected_circles, detected_rectangles = detect_shapes(nodes_mask, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "77c3937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialated_circles = [dilate_contour(c, img_no_text.shape, config) for c in detected_circles]\n",
    "dialated_rectangles = [dilate_contour(r, img_no_text.shape, config) for r in detected_rectangles]\n",
    "img_no_shapes = remove_contours(img_empty_nodes_filled, dialated_circles + dialated_rectangles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "048e9221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_drawn = img_color_resized.copy() # Convert to BGR for drawing\n",
    "img_drawn = cv2.cvtColor(img_no_shapes, cv2.COLOR_GRAY2BGR) # Convert to BGR for drawing\n",
    "for contour in detected_circles + detected_rectangles:\n",
    "    cv2.drawContours(img_drawn, [contour], -1, (0, 255, 0), 2) # Draw contours in green\n",
    "Image.fromarray(img_drawn).show(title=\"Detected Shapes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "c53e08bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(img_no_shapes).show(title=\"Image without shapes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "9235e6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(detected_circles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "978304ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "places = [Place.from_contour(circle) for circle in detected_circles]\n",
    "transitions = [Transition.from_contour(rect) for rect in detected_rectangles]\n",
    "   \n",
    "len(transitions), len(detected_rectangles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "455d15f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### draw places and transitions\n",
    "img_drawn = cv2.cvtColor(img_no_shapes, cv2.COLOR_GRAY2BGR) # Convert to BGR for drawing\n",
    "for place in places:\n",
    "    cv2.circle(img_drawn, (place.center.x, place.center.y), int(place.radius), (255, 0, 0), 2) # Draw places in blue\n",
    "for transition in transitions:\n",
    "    cv2.drawContours(img_drawn, [transition.box_points.astype(np.int32)], -1, (0, 255, 0), 2) # Draw transitions in green\n",
    "\n",
    "Image.fromarray(img_drawn).show(title=\"Detected Places and Transitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "029135da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0, 255], dtype=uint8)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(skeleton.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "47eed4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skeleton = skeletonize(img_no_shapes / 255).astype(np.uint8)*255\n",
    "# img_draw = cv2.cvtColor(skeleton.copy(), cv2.COLOR_GRAY2BGR)\n",
    "# hough_lines = cv2.HoughLinesP(skeleton, 1, np.pi/180, 15, minLineLength=10, maxLineGap=25)\n",
    "# for line in hough_lines:\n",
    "#     cv2.line(img_draw, (line[0][0], line[0][1]), (line[0][2], line[0][3]), (np.random.randint(0, 256), np.random.randint(0, 256), np.random.randint(0, 256)), 2)\n",
    "# Image.fromarray(img_draw).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "a49b0451",
   "metadata": {},
   "outputs": [],
   "source": [
    "hough_lines = get_hough_lines(img_no_shapes, config)\n",
    "img_draw = cv2.cvtColor(skeletonize(img_no_shapes / 255).astype(np.uint8)*255, cv2.COLOR_GRAY2BGR)\n",
    "for line in hough_lines:\n",
    "    cv2.line(img_draw, (line[0][0], line[0][1]), (line[0][2], line[0][3]), (np.random.randint(0, 256), np.random.randint(0, 256), np.random.randint(0, 256)), 2)\n",
    "Image.fromarray(img_draw).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "29ffeac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 59, 59)"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hough_bundler_min_distance = config.get('connection_processing', {}).get('hough_bundler_min_distance', 10)\n",
    "hough_bundler_min_angle = config.get('connection_processing', {}).get('hough_bundler_min_angle', 5) \n",
    "\n",
    "bundler = HoughBundler(min_distance = hough_bundler_min_distance ,min_angle=hough_bundler_min_angle)\n",
    "merged_hough_lines = bundler.process_lines(hough_lines)\n",
    "\n",
    "lines = [Line(Point(line[0][0], line[0][1]), Point(line[0][2], line[0][3])) for line in merged_hough_lines]\n",
    "len(hough_lines), len(merged_hough_lines), len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "0581891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_drawn = cv2.cvtColor(img_no_shapes, cv2.COLOR_GRAY2BGR) # Convert to BGR for drawing\n",
    "for line in lines:\n",
    "    cv2.line(img_drawn, (line.point1.x, line.point1.y), (line.point2.x, line.point2.y), (255, 0, 0), 2) # Draw lines in blue\n",
    "\n",
    "Image.fromarray(img_drawn).show(title=\"Detected Places and Transitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "c8dc9303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_with_proximity = list(filter(lambda x: x.proximity_node, [point for line in lines for point in [line.point1, line.point2]]))\n",
    "len(points_with_proximity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "2b0ba10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(places[0], Place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "58679331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, 90)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_lines, processed_places, processed_transitions = assign_proximity_nodes(\n",
    "    lines, \n",
    "    places, \n",
    "    transitions, \n",
    "    config\n",
    ")\n",
    "entry_points = get_entry_points_from_lines(processed_lines)\n",
    "\n",
    "points_with_proximity = list(filter(lambda x: x.proximity_node, [point for line in processed_lines for point in [line.point1, line.point2]]))\n",
    "len(processed_lines), len(points_with_proximity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "26e01698",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_drawn = cv2.cvtColor(img_no_shapes, cv2.COLOR_GRAY2BGR) \n",
    "for line in processed_lines:\n",
    "\n",
    "    color = (np.random.randint(0, 256), np.random.randint(0, 256), np.random.randint(0, 256))\n",
    "    cv2.line(img_drawn, (line.point1.x, line.point1.y), (line.point2.x, line.point2.y), color , 2) # Draw lines in blue\n",
    "\n",
    "for point in entry_points:\n",
    "    cv2.circle(img_drawn, (point.x, point.y), 5, (0, 255, 0), -1) # Draw proximity points in green\n",
    "\n",
    "Image.fromarray(img_drawn).show(title=\"Detected Places and Transitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "41855e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "### filtered lines still includes lines that are not connected to any proximity node\n",
    "filtered_lines = []\n",
    "for line in processed_lines:\n",
    "    if line.point1.proximity_node == line.point2.proximity_node != None:\n",
    "        continue\n",
    "    else:\n",
    "        filtered_lines.append(line)\n",
    "        \n",
    "len(filtered_lines)\n",
    "img_draw = cv2.cvtColor(np.zeros_like(img_no_shapes), cv2.COLOR_GRAY2BGR)\n",
    "for line in filtered_lines:\n",
    "    color = (np.random.randint(0, 256), np.random.randint(0, 256), np.random.randint(0, 256))\n",
    "    \n",
    "    cv2.line(img_draw, (line.point1.x, line.point1.y), (line.point2.x, line.point2.y), color, 2)\n",
    "# for point in entry_points:\n",
    "#     cv2.circle(img_draw, (point.x, point.y), 5, (0, 255, 0), -1) # Draw proximity points in green\n",
    "\n",
    "Image.fromarray(img_draw).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "9555e302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: Arrays of 2-dimensional vectors are deprecated. Use arrays of 3-dimensional vectors instead. (deprecated in NumPy 2.0)\n"
     ]
    }
   ],
   "source": [
    "found_paths_result = find_line_paths(\n",
    "    filtered_lines,\n",
    "    proximity_threshold=100.0, # Max distance between points to consider connecting\n",
    "    dot_product_weight=0.5,\n",
    "    distance_to_line_weight=0.25,\n",
    "    endpoint_distance_weight=0.25\n",
    ")\n",
    "img_draw = cv2.cvtColor(np.zeros_like(img_no_shapes), cv2.COLOR_GRAY2BGR)\n",
    "for path in found_paths_result:\n",
    "    color = (np.random.randint(0, 256), np.random.randint(0, 256), np.random.randint(0, 256))\n",
    "    for line in path[\"lines\"]:\n",
    "        cv2.line(img_draw, (line.point1.x, line.point1.y), (line.point2.x, line.point2.y), color, 2)\n",
    "\n",
    "Image.fromarray(img_draw).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "c15112d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrowhead_result = detect_arrowheads(\n",
    "    image=img_color_resized,\n",
    "    config=config\n",
    ")\n",
    "detections = sv.Detections.from_inference(arrowhead_result)\n",
    "arrow_contours = [minmaxToContours(xyxy) for xyxy in detections.xyxy]\n",
    "# img_no_arrows = remove_contours(img_empty_nodes_filled, arrow_contours)\n",
    "# Image.fromarray(img_no_arrows).show(title=\"Image with Arrowheads Removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "4199efe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arrowhead_result[\"predictions\"]), len(arrow_contours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "149e2a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No closest point found for the arrowhead center.\n",
      "No closest point found for the arrowhead center.\n",
      "No closest point found for the arrowhead center.\n",
      "No closest point found for the arrowhead center.\n",
      "No closest point found for the arrowhead center.\n",
      "No closest point found for the arrowhead center.\n"
     ]
    }
   ],
   "source": [
    "paths_with_arrows, rejected_arrows_count = assign_arrowheads(found_paths_result, arrowhead_result, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "16133436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 34, 6)"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_with_arrows_before = list(filter(lambda x: x.is_arrow, [point for path in found_paths_result for line in path[\"lines\"] for point in [line.point1, line.point2]]))\n",
    "points_with_arrows_after = list(filter(lambda x: x.is_arrow, [point for path in paths_with_arrows for line in path[\"lines\"] for point in [line.point1, line.point2]]))\n",
    "len(points_with_arrows_before), len(points_with_arrows_after), rejected_arrows_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "71f7f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_draw = cv2.cvtColor(np.zeros_like(img_no_shapes), cv2.COLOR_GRAY2BGR)\n",
    "for path in paths_with_arrows:\n",
    "    color = (np.random.randint(0, 256), np.random.randint(0, 256), np.random.randint(0, 256))\n",
    "    for line in path[\"lines\"]:\n",
    "        cv2.line(img_draw, (line.point1.x, line.point1.y), (line.point2.x, line.point2.y), color, 2)\n",
    "\n",
    "Image.fromarray(img_draw).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "1fec5df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Arc(source=Place(center=Point(1896, 194), radius=42.63063049316406), target=Transition(center=Point(2046, 97), height=106.0, width=18.0, angle=90.0)),\n",
       "  Arc(source=Transition(center=Point(1233, 307), height=106.0, width=28.0, angle=90.0), target=Place(center=Point(1505, 559), radius=43.1812858581543)),\n",
       "  Arc(source=Place(center=Point(2047, 454), radius=42.58375930786133), target=Transition(center=Point(1233, 307), height=106.0, width=28.0, angle=90.0)),\n",
       "  Arc(source=Place(center=Point(396, 643), radius=43.05750274658203), target=Transition(center=Point(600, 483), height=106.0, width=20.0, angle=90.0)),\n",
       "  Arc(source=Transition(center=Point(2046, 97), height=106.0, width=18.0, angle=90.0), target=Place(center=Point(2047, 454), radius=42.58375930786133))],\n",
       " 41)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arcs = get_arcs(paths_with_arrows)\n",
    "arcs[:5], len(arcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "884d1fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### check arcs for errors\n",
    "### filter arcs to remove cycles from the same source and target\n",
    "arcs_filtered = []\n",
    "for arc in arcs:\n",
    "    if arc.source != arc.target and type(arc.source) != type(arc.target):\n",
    "        arcs_filtered.append(arc)\n",
    "\n",
    "len(arcs_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "95fe365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### visualize arcs\n",
    "img_draw = cv2.cvtColor(np.zeros_like(img_no_shapes), cv2.COLOR_GRAY2RGB)\n",
    "for arc in arcs_filtered:\n",
    "    src_color = (0, 0, 255) # Red for source\n",
    "    tgt_color = (255, 0, 0) # Blue for target\n",
    "    \n",
    "    cv2.circle(img_draw, (arc.start_point.x, arc.start_point.y), 5, src_color, -1) # Draw source point\n",
    "    cv2.circle(img_draw, (arc.end_point.x, arc.end_point.y), 5, tgt_color, -1) # Draw target point\n",
    "\n",
    "    cv2.line(img_draw, (arc.start_point.x, arc.start_point.y), (arc.end_point.x, arc.end_point.y), (0, 255, 0), 2) # Draw line in green\n",
    "\n",
    "Image.fromarray(img_draw).show(title=\"Detected Arcs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "a5ae4dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_contour = np.array(([point.get_numpy_array() for point in arcs[0].points])).reshape((-1, 1, 2))\n",
    "place_contour = places[0].original_detection_data\n",
    "transition_contour = transitions[0].original_detection_data\n",
    "\n",
    "arc_contour.shape, place_contour.shape, transition_contour.shape\n",
    "\n",
    "# Define a sample point\n",
    "my_point = Point(50, 50)\n",
    "\n",
    "img_draw = cv2.cvtColor(np.zeros_like(img_no_shapes), cv2.COLOR_GRAY2BGR)\n",
    "cv2.circle(img_draw, (my_point.x, my_point.y), 5, (0, 255, 0), -1) # Draw point in green\n",
    "cv2.polylines(img_draw, [arc_contour], isClosed=False, color=(0, 255, 0), thickness=2) # Draw polyline in green\n",
    "cv2.polylines(img_draw, [place_contour], isClosed=True, color=(255, 0, 0), thickness=2) # Draw place in blue\n",
    "cv2.polylines(img_draw, [transition_contour], isClosed=True, color=(0, 0, 255), thickness=2) # Draw transition in red\n",
    "Image.fromarray(img_draw).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "6f608ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from Point(50, 50) to arc contour: 1890.6237066111278\n",
      "Distance from Point(50, 50) to place contour: 1643.8795576318844\n",
      "Distance from Point(50, 50) to transition contour: 1344.5463919106696\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dist_to_arc = find_closest_distance_to_contour(my_point, arc_contour)\n",
    "dist_to_place = find_closest_distance_to_contour(my_point, place_contour)\n",
    "dist_to_transition = find_closest_distance_to_contour(my_point, transition_contour)\n",
    "print(f\"Distance from {my_point} to arc contour: {dist_to_arc}\")\n",
    "print(f\"Distance from {my_point} to place contour: {dist_to_place}\")\n",
    "print(f\"Distance from {my_point} to transition contour: {dist_to_transition}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "ddd34c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Associated Text ---\n",
      "Place(center=Point(1303, 1178), radius=42.83219909667969) has text: ['0']\n",
      "Place(center=Point(1928, 987), radius=42.33136749267578) has text: ['deadend', '0']\n",
      "Place(center=Point(849, 772), radius=42.84130096435547) has text: ['0']\n",
      "Place(center=Point(1506, 764), radius=42.801334381103516) has text: ['orders', '0']\n",
      "Place(center=Point(2052, 744), radius=42.6140251159668) has text: ['P1', '0']\n",
      "Place(center=Point(396, 643), radius=43.05750274658203) has text: ['customers', '0']\n",
      "Place(center=Point(86, 643), radius=42.58401870727539) has text: ['P1', '1']\n",
      "Place(center=Point(1505, 559), radius=43.1812858581543) has text: ['orders', '0']\n",
      "Place(center=Point(795, 471), radius=42.31279373168945) has text: ['U']\n",
      "Place(center=Point(2047, 454), radius=42.58375930786133) has text: ['0']\n",
      "Place(center=Point(812, 296), radius=43.51975631713867) has text: ['size', '72']\n",
      "Place(center=Point(1896, 194), radius=42.63063049316406) has text: ['P1', '1']\n",
      "Place(center=Point(425, 111), radius=42.585105895996094) has text: ['P1', '1']\n",
      "Place(center=Point(765, 105), radius=42.90250778198242) has text: ['0']\n",
      "Place(center=Point(1651, 103), radius=42.65388870239258) has text: ['0']\n",
      "Transition(center=Point(603, 773), height=106.0, width=18.0, angle=90.0) has text: ['t=0.0']\n",
      "Transition(center=Point(1099, 764), height=106.0, width=18.0, angle=90.0) has text: ['t-0.0']\n",
      "Transition(center=Point(241, 643), height=107.99998474121094, width=18.999996185302734, angle=90.0) has text: ['T1']\n",
      "Transition(center=Point(1229, 524), height=106.0, width=20.0, angle=90.0) has text: ['order']\n",
      "Transition(center=Point(579, 106), height=106.0, width=20.0, angle=90.0) has text: ['T1']\n",
      "Transition(center=Point(1005, 103), height=108.0, width=39.0, angle=90.0) has text: ['size']\n",
      "Arc(source=Transition(center=Point(1233, 307), height=106.0, width=28.0, angle=90.0), target=Place(center=Point(1505, 559), radius=43.1812858581543)) has text: ['cancelled,']\n",
      "Arc(source=Place(center=Point(2047, 454), radius=42.58375930786133), target=Transition(center=Point(1233, 307), height=106.0, width=28.0, angle=90.0)) has text: ['te0:0-']\n",
      "Arc(source=Transition(center=Point(2046, 97), height=106.0, width=18.0, angle=90.0), target=Place(center=Point(2047, 454), radius=42.58375930786133)) has text: ['t-0.0', 'b=p']\n",
      "Arc(source=Place(center=Point(396, 643), radius=43.05750274658203), target=Transition(center=Point(603, 773), height=106.0, width=18.0, angle=90.0)) has text: ['fridgpot_avaliable']\n",
      "Arc(source=Transition(center=Point(579, 106), height=106.0, width=20.0, angle=90.0), target=Place(center=Point(765, 105), radius=42.90250778198242)) has text: ['72']\n",
      "Arc(source=Place(center=Point(765, 105), radius=42.90250778198242), target=Transition(center=Point(1229, 524), height=106.0, width=20.0, angle=90.0)) has text: ['create_shep']\n",
      "Arc(source=Transition(center=Point(1229, 524), height=106.0, width=20.0, angle=90.0), target=Place(center=Point(1506, 764), radius=42.801334381103516)) has text: ['purchace']\n",
      "Arc(source=Transition(center=Point(1901, 351), height=108.0, width=20.0, angle=90.0), target=Place(center=Point(1896, 194), radius=42.63063049316406)) has text: ['T5']\n",
      "Arc(source=Place(center=Point(1651, 103), radius=42.65388870239258), target=Transition(center=Point(2046, 97), height=106.0, width=18.0, angle=90.0)) has text: ['19']\n",
      "Arc(source=Transition(center=Point(1099, 764), height=106.0, width=18.0, angle=90.0), target=Place(center=Point(1651, 103), radius=42.65388870239258)) has text: ['t-00', 'b=0']\n",
      "Arc(source=Transition(center=Point(1058, 1003), height=108.0, width=20.0, angle=90.0), target=Place(center=Point(1303, 1178), radius=42.83219909667969)) has text: ['lostcustomers']\n",
      "Arc(source=Place(center=Point(849, 772), radius=42.84130096435547), target=Transition(center=Point(1058, 1003), height=108.0, width=20.0, angle=90.0)) has text: ['go.to_omer_store']\n",
      "Arc(source=Transition(center=Point(1901, 351), height=108.0, width=20.0, angle=90.0), target=Place(center=Point(2047, 454), radius=42.58375930786133)) has text: ['is_cancelled']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: Arrays of 2-dimensional vectors are deprecated. Use arrays of 3-dimensional vectors instead. (deprecated in NumPy 2.0)\n"
     ]
    }
   ],
   "source": [
    "# Set a threshold\n",
    "text_linking_threshold = config.get('connection_processing', {}).get('text_linking_threshold',  25.0 )\n",
    "\n",
    "link_text_to_elements(\n",
    "    detected_text_list,\n",
    "    places,\n",
    "    transitions,\n",
    "    arcs_filtered,\n",
    "    text_linking_threshold\n",
    ")\n",
    "\n",
    "# Check associations\n",
    "print(\"--- Associated Text ---\")\n",
    "for p in places:\n",
    "    if p.text: print(f\"{p} has text: {[t.value for t in p.text]}\")\n",
    "for t in transitions:\n",
    "    if t.text: print(f\"{t} has text: {[txt.value for txt in t.text]}\") # changed t.text to txt.value\n",
    "for a in arcs_filtered:\n",
    "    if a.text: print(f\"{a} has text: {[t.value for t in a.text]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "acd88d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Place Place(center=Point(1303, 1178), radius=42.83219909667969) has 0 markers.\n",
      "Place Place(center=Point(1298, 999), radius=42.35411071777344) has 0 markers.\n",
      "Place Place(center=Point(1928, 987), radius=42.33136749267578) has 0 markers.\n",
      "Place Place(center=Point(849, 772), radius=42.84130096435547) has 0 markers.\n",
      "Place Place(center=Point(1506, 764), radius=42.801334381103516) has 0 markers.\n",
      "Place Place(center=Point(2052, 744), radius=42.6140251159668) has 0 markers.\n",
      "Place Place(center=Point(396, 643), radius=43.05750274658203) has 0 markers.\n",
      "Place Place(center=Point(86, 643), radius=42.58401870727539) has 1 markers.\n",
      "Place Place(center=Point(1505, 559), radius=43.1812858581543) has 0 markers.\n",
      "Place Place(center=Point(795, 471), radius=42.31279373168945) has 0 markers.\n",
      "Place Place(center=Point(2047, 454), radius=42.58375930786133) has 0 markers.\n",
      "Place Place(center=Point(812, 296), radius=43.51975631713867) has 72 markers.\n",
      "Place Place(center=Point(1896, 194), radius=42.63063049316406) has 1 markers.\n",
      "Place Place(center=Point(425, 111), radius=42.585105895996094) has 1 markers.\n",
      "Place Place(center=Point(1186, 104), radius=42.285667419433594) has 0 markers.\n",
      "Place Place(center=Point(765, 105), radius=42.90250778198242) has 0 markers.\n",
      "Place Place(center=Point(1651, 103), radius=42.65388870239258) has 0 markers.\n"
     ]
    }
   ],
   "source": [
    "for place in places:\n",
    "    place.update_markers_from_text()\n",
    "    print(f\"Place {place} has {place.markers} markers.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diploma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
